{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# How to retrieve the requisite data\n",
    "\n",
    "+ This notebook generates cooccurrence counts given Web of Science output.\n",
    "+ No Web of Science output is included in knowknow in compliance with Web of Science terms of use.\n",
    "+ **This notebook is only useful if you have your own Web of Science output**, and want to run analyses on that.\n",
    "\n",
    "Data can be downloaded from any Web of Science search results page:\n",
    "1. `Export -> Other File Formats`. \n",
    "2. In the dropdowns, specify `Record Content -> Full Record and Cited References` and `File Format -> Tab Delimited (Win)`\n",
    "3. Type the folder containing the saved records as `.txt` files into the variable `wos_base` below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "1. Edit the parameters to fit your needs\n",
    "2. Run all cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wos_base = \"path/to/wos/data\"\n",
    "wos_base = \"G:/My Drive/projects/qualitative analysis of literature/pre 5-12-2020/009 get everything from WOS\"\n",
    "journal_keep = [\"ETHNIC AND RACIAL STUDIES\", \"LAW & SOCIETY REVIEW\", \"DISCOURSE & SOCIETY\", \"SOCIOLOGICAL INQUIRY\", \"CONTRIBUTIONS TO INDIAN SOCIOLOGY\", \"SOCIETY & NATURAL RESOURCES\", \"RATIONALITY AND SOCIETY\", \"DEVIANT BEHAVIOR\", \"ACTA SOCIOLOGICA\", \"SOCIOLOGY-THE JOURNAL OF THE BRITISH SOCIOLOGICAL ASSOCIATION\", \"WORK EMPLOYMENT AND SOCIETY\", \"SOCIOLOGICAL METHODS & RESEARCH\", \"SOCIOLOGICAL PERSPECTIVES\", \"JOURNAL OF MARRIAGE AND FAMILY\", \"WORK AND OCCUPATIONS\", \"JOURNAL OF CONTEMPORARY ETHNOGRAPHY\", \"THEORY AND SOCIETY\", \"POLITICS & SOCIETY\", \"SOCIOLOGICAL SPECTRUM\", \"RACE & CLASS\", \"ANTHROZOOS\", \"LEISURE SCIENCES\", \"COMPARATIVE STUDIES IN SOCIETY AND HISTORY\", \"SOCIAL SCIENCE QUARTERLY\", \"MEDIA CULTURE & SOCIETY\", \"SOCIOLOGY OF HEALTH & ILLNESS\", \"SOCIOLOGIA RURALIS\", \"SOCIOLOGICAL REVIEW\", \"TEACHING SOCIOLOGY\", \"BRITISH JOURNAL OF SOCIOLOGY\", \"JOURNAL OF THE HISTORY OF SEXUALITY\", \"SOCIOLOGY OF EDUCATION\", \"SOCIAL NETWORKS\", \"ARMED FORCES & SOCIETY\", \"YOUTH & SOCIETY\", \"POPULATION AND DEVELOPMENT REVIEW\", \"SOCIETY\", \"JOURNAL OF HISTORICAL SOCIOLOGY\", \"HUMAN ECOLOGY\", \"INTERNATIONAL SOCIOLOGY\", \"SOCIAL FORCES\", \"EUROPEAN SOCIOLOGICAL REVIEW\", \"JOURNAL OF HEALTH AND SOCIAL BEHAVIOR\", \"SOCIOLOGICAL THEORY\", \"SOCIAL INDICATORS RESEARCH\", \"POETICS\", \"HUMAN STUDIES\", \"SOCIOLOGICAL FORUM\", \"AMERICAN SOCIOLOGICAL REVIEW\", \"SOCIOLOGY OF SPORT JOURNAL\", \"SOCIOLOGY OF RELIGION\", \"JOURNAL OF LAW AND SOCIETY\", \"GENDER & SOCIETY\", \"BRITISH JOURNAL OF SOCIOLOGY OF EDUCATION\", \"LANGUAGE IN SOCIETY\", \"AMERICAN JOURNAL OF ECONOMICS AND SOCIOLOGY\", \"ANNALS OF TOURISM RESEARCH\", \"SOCIAL PROBLEMS\", \"INTERNATIONAL JOURNAL OF INTERCULTURAL RELATIONS\", \"SOCIAL SCIENCE RESEARCH\", \"SYMBOLIC INTERACTION\", \"JOURNAL OF LEISURE RESEARCH\", \"ECONOMY AND SOCIETY\", \"INTERNATIONAL JOURNAL OF COMPARATIVE SOCIOLOGY\", \"SOCIAL COMPASS\", \"SOCIOLOGICAL QUARTERLY\", \"JOURNAL OF MATHEMATICAL SOCIOLOGY\", \"AMERICAN JOURNAL OF SOCIOLOGY\", \"REVIEW OF RELIGIOUS RESEARCH\", \"RURAL SOCIOLOGY\", \"JOURNAL FOR THE SCIENTIFIC STUDY OF RELIGION\", \"ARCHIVES EUROPEENNES DE SOCIOLOGIE\", \"CANADIAN JOURNAL OF SOCIOLOGY-CAHIERS CANADIENS DE SOCIOLOGIE\"]\n",
    "name_blacklist = [\n",
    "    \"*us\", 'Press', 'Knopf', '(January', 'Co', 'London', 'Bros', 'Books', 'Wilson','[anonymous]'\n",
    "]\n",
    "\n",
    "debug = False\n",
    "database_name = \"sociology-wos\"\n",
    "\n",
    "RUN_PREGROUP = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(_dh[0].split(\"knowknow\")[0])\n",
    "from knowknow import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First pass - just counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import DictReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcount = 0\n",
    "total_inserts = 0\n",
    "to_inserts = []\n",
    "basedir = Path(wos_base)\n",
    "\n",
    "# keeps track of all the years seen for grouped citations\n",
    "multi_year = defaultdict(lambda: defaultdict(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiating counters\n",
    "cnt_ind = defaultdict(lambda:defaultdict(int))\n",
    "track_doc = defaultdict(lambda:defaultdict(set))\n",
    "cnt_doc = defaultdict(lambda:defaultdict(int))\n",
    "\n",
    "def cnt(term, space, doc):\n",
    "    # it's a set, yo\n",
    "    track_doc[space][term].add(doc)\n",
    "    # update cnt_doc\n",
    "    cnt_doc[space][term] = len(track_doc[space][term])\n",
    "    # update ind count\n",
    "    cnt_ind[space][term] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This cell ensures there are not overflow errors while importing large CSVs\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "maxInt = sys.maxsize\n",
    "\n",
    "while True:\n",
    "    # decrease the maxInt value by factor 10\n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fix_auths(auths):\n",
    "    for a in auths:\n",
    "        a = a.strip()\n",
    "        if not len(a):\n",
    "            continue\n",
    "\n",
    "        a = a.split()[0].lower()\n",
    "        yield a\n",
    "\n",
    "\n",
    "def fix_refs(refs):\n",
    "    for r in refs:\n",
    "        yspl = re.split(\"((?:18|19|20)[0-9]{2})\", r)\n",
    "\n",
    "        if len(yspl) < 2:\n",
    "            continue\n",
    "\n",
    "        auth, year = yspl[:2]\n",
    "        auth = auth.strip()\n",
    "        year = int(year)\n",
    "\n",
    "        if auth == \"\":\n",
    "            continue\n",
    "\n",
    "        auth = \"&\".join( fix_auths( auth.split(\",\") ) )\n",
    "\n",
    "        yield ( auth, year, r )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-group cited references while counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groups successfully loaded.\n",
      "Loaded keys: dict_keys(['c'])\n",
      "Available keys: ['fj', 'fy', 'ty', 'fy.ty', 'fj.ty', 'fj.fy', 'a', 'ay', 'aj', 'ajy', 'ac', 'c', 'cy', 'cj', 'cjy']\n"
     ]
    }
   ],
   "source": [
    "groups=None\n",
    "\n",
    "if RUN_PREGROUP:\n",
    "    # loading precomputed groupings of cited books and articles, if the grouping has been generated\n",
    "\n",
    "    try:\n",
    "        groups = load_variable(\"%s.groups\" % database_name)\n",
    "        print(\"Groups successfully loaded.\")\n",
    "\n",
    "    except VariableNotFound:\n",
    "        print(\"Groups don't exist yet. It's important to incorporate a fuzzy match for cited references.\"\n",
    "              \"Run this script once, then run 'trend summaries/cysum.ipynb' to generate summaries and filter out small cited works.\"\n",
    "              \"Then run 'grouping article and book names.ipynb' to generate groupings.\"\n",
    "              \"Then run this notebook again to generate counts for grouped entries.\"\n",
    "              \"And finally, to make sure cysum is up to date, run 'trend summaries/cysum.ipynb' one more time.\"\n",
    "             )\n",
    "\n",
    "    if groups is not None:\n",
    "        from collections import defaultdict\n",
    "\n",
    "        # find the most popular representation\n",
    "        current_c = get_cnt(\"%s.doc\"%database_name, ['c'])\n",
    "\n",
    "        def get_reps(groups):\n",
    "            ret = defaultdict(set)\n",
    "            for k,v in groups.items():\n",
    "                ret[v].add(k)\n",
    "            ret = {\n",
    "                k: max(v, key=lambda x:current_c['c'][x])\n",
    "                for k,v in ret.items()\n",
    "            }\n",
    "            return ret\n",
    "\n",
    "        group_reps = get_reps(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 0/386: 74501-75000.txt\n",
      "Document 10000\n",
      "File 50/386: 3501-4000.txt\n",
      "Document 20000\n",
      "Document 30000\n",
      "File 100/386: 28501-29000.txt\n",
      "Document 40000\n",
      "File 150/386: 53501-54000.txt\n",
      "Document 50000\n",
      "Document 60000\n",
      "File 200/386: 4501-5000.txt\n",
      "Document 70000\n",
      "Document 80000\n",
      "File 250/386: 29501-30000.txt\n",
      "Document 90000\n",
      "Document 100000\n",
      "File 300/386: 54501-55000.txt\n",
      "Document 110000\n",
      "Document 120000\n",
      "File 350/386: 79501-80000.txt\n",
      "Document 130000\n"
     ]
    }
   ],
   "source": [
    "# processes WoS txt output files one by one, counting relevant cooccurrences as it goes\n",
    "\n",
    "for i, f in enumerate( list(basedir.glob(\"**/*.txt\")) ):\n",
    "\n",
    "    with f.open(encoding='utf8') as pfile:\n",
    "        r = DictReader(pfile, delimiter=\"\\t\")\n",
    "        rows = list(r)\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(\"File %s/%s: %s\" % (i, len(list(basedir.glob(\"**/*.txt\"))),f.name))\n",
    "\n",
    "    for i, r in enumerate(rows):\n",
    "\n",
    "        if r['DT'] != \"Article\":\n",
    "            continue\n",
    "\n",
    "        refs = r[\"CR\"].strip().split(\";\")\n",
    "        refs = list( fix_refs(refs) )\n",
    "\n",
    "        if not len(refs):\n",
    "            continue\n",
    "\n",
    "        #print(refs)\n",
    "\n",
    "        dcount += 1\n",
    "        if dcount % 10000 == 0:\n",
    "            print(\"Document %s\" % dcount)\n",
    "\n",
    "\n",
    "        authors = r['AU'].split(\";\")\n",
    "        authors = [x.strip() for x in authors]\n",
    "\n",
    "        if False:\n",
    "            for i in range(10):\n",
    "                print(\"-\"*20)\n",
    "\n",
    "\n",
    "        uid = r['UT']\n",
    "\n",
    "\n",
    "        try:\n",
    "            int(r['PY'])\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "        for (auth,year,full_ref) in refs:\n",
    "            if debug:\n",
    "                print(full_ref)\n",
    "            if 'DOI' not in full_ref and not len(re.findall(r', V[0-9]+, P[0-9]+', full_ref)):\n",
    "                # splits off the author and year, and takes until the next comma\n",
    "                full_ref = \"|\".join(\n",
    "                    [auth]+\n",
    "                    [x.strip().lower() for x in full_ref.split(\",\")[2:3]]\n",
    "                )\n",
    "            else:\n",
    "                # just adds a fixed name and date to the front\n",
    "                full_ref = \"|\".join(\n",
    "                    [auth, str(year)] +\n",
    "                    [\",\".join( full_ref.strip().lower().split(\",\")[2:] ).split(\", doi\")[0]]\n",
    "                )\n",
    "                \n",
    "            if groups is not None:\n",
    "                if full_ref in groups:\n",
    "                    # retrieves retrospectively-computed groups\n",
    "                    full_ref = group_reps[\n",
    "                        groups[full_ref]\n",
    "                    ]\n",
    "                else:\n",
    "                    # a small minority, the ones which are dropped in this process anyways\n",
    "                    continue\n",
    "\n",
    "                if debug:\n",
    "                    print(full_ref)\n",
    "                    print(\"--------------------\")\n",
    "                    \n",
    "            \n",
    "            ref = (auth,year)\n",
    "\n",
    "            ref_str = \"|\".join( str(x) for x in ref )\n",
    "            ref_str = full_ref.strip()\n",
    "\n",
    "            if ref[0] in name_blacklist:\n",
    "                continue\n",
    "            if \"*\" in ref[0]:\n",
    "                continue\n",
    "                \n",
    "            if r['SO'] not in journal_keep:\n",
    "                continue\n",
    "                \n",
    "            # BEGIN COUNTING!!!\n",
    "            \n",
    "            multi_year[full_ref][year] += 1\n",
    "                \n",
    "            cnt(r['SO'], 'fj', uid)\n",
    "            cnt(int(r['PY']), 'fy', uid)\n",
    "            cnt(ref[1], 'ty', uid)\n",
    "            cnt((int(r['PY']), year), 'fy.ty', uid)\n",
    "            cnt((r['SO'], year), 'fj.ty', uid)\n",
    "\n",
    "            cnt((r['SO'],int(r['PY'])), 'fj.fy', uid)\n",
    "\n",
    "            for a in authors:\n",
    "                cnt(a, 'a', uid)\n",
    "                cnt((a,int(r['PY'])), 'a.fy', uid)\n",
    "                cnt((a,r['SO']), 'a.fj', uid)\n",
    "                cnt((a,r['SO'], int(r['PY'])), 'a.fj.fy', uid)\n",
    "\n",
    "                cnt((a,ref_str), 'a.c', uid)\n",
    "\n",
    "            cnt(ref_str, 'c', uid)\n",
    "            cnt((ref_str, int(r['PY'])), 'c.fy', uid)\n",
    "            cnt((ref_str, r['SO']), 'c.fj', uid)\n",
    "            cnt((ref_str, int(r['PY']), r['SO']), 'c.fy.j', uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cocitation counter\n",
    "\n",
    "Because there are so many cited works, a full cocitation network would be prohibitively large to compute and store on disk. \n",
    "Furthermore, the full network is not very useful.\n",
    "The following creates a cocitation network among only the most common 1000 cited works.\n",
    "It counts the number of times `cnt['cc'][ (c1,c2) ]` that `c1` and `c2` are cited together in a work.\n",
    "The `ind` and `doc` counters are identical for this counter, `'cc'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# allowed references for cocitation analysis: 1000\n",
      "Examples: ['mead|mind self soc', 'coleman|intro mathematical s', 'aiken|multiple regression']\n",
      "File 0/191: G:\\My Drive\\projects\\qualitative analysis of literature\\pre 5-12-2020\\009 get everything from WOS\\74501-75000.txt\n",
      "File 50/191: G:\\My Drive\\projects\\qualitative analysis of literature\\pre 5-12-2020\\009 get everything from WOS\\3501-4000.txt\n",
      "File 100/191: G:\\My Drive\\projects\\qualitative analysis of literature\\pre 5-12-2020\\009 get everything from WOS\\28501-29000.txt\n",
      "File 150/191: G:\\My Drive\\projects\\qualitative analysis of literature\\pre 5-12-2020\\009 get everything from WOS\\53501-54000.txt\n",
      "File 200/191: G:\\My Drive\\projects\\qualitative analysis of literature\\pre 5-12-2020\\009 get everything from WOS\\2006 and back\\4501-5000.txt\n",
      "File 250/191: G:\\My Drive\\projects\\qualitative analysis of literature\\pre 5-12-2020\\009 get everything from WOS\\2006 and back\\29501-30000.txt\n",
      "File 300/191: G:\\My Drive\\projects\\qualitative analysis of literature\\pre 5-12-2020\\009 get everything from WOS\\2006 and back\\54501-55000.txt\n",
      "File 350/191: G:\\My Drive\\projects\\qualitative analysis of literature\\pre 5-12-2020\\009 get everything from WOS\\2006 and back\\79501-80000.txt\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "allowed_refs = Counter(dict(cnt_ind['c'].items())).most_common(1000)\n",
    "allowed_refs = set( x[0] for x in allowed_refs )\n",
    "\n",
    "print(\"# allowed references for cocitation analysis: %s\" % len(allowed_refs))\n",
    "print(\"Examples: %s\" % str(list(allowed_refs)[:3]))\n",
    "\n",
    "# enumerating cocitation for works with at least 10 citations\n",
    "dcount = 0\n",
    "refcount = 0\n",
    "\n",
    "for i, f in enumerate(list(basedir.glob(\"**/*.txt\"))):\n",
    "\n",
    "    with f.open(encoding='utf8') as pfile:\n",
    "        r = DictReader(pfile, delimiter=\"\\t\")\n",
    "        rows = list(r)\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(\"File %s/%s: %s\" % (i, len(list(basedir.glob(\"**/*.txt\"))), f))\n",
    "\n",
    "    for i, r in enumerate(rows):\n",
    "\n",
    "        if r['DT'] != \"Article\":\n",
    "            continue\n",
    "\n",
    "        refs = r[\"CR\"].strip().split(\";\")\n",
    "        refs = list(fix_refs(refs))\n",
    "\n",
    "        if not len(refs):\n",
    "            continue\n",
    "\n",
    "        uid = r['UT']\n",
    "\n",
    "        try:\n",
    "            int(r['PY'])\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        for ref in refs:\n",
    "\n",
    "            ref_str = \"|\".join( str(x) for x in ref )\n",
    "            if ref_str not in allowed_refs:\n",
    "                continue\n",
    "\n",
    "            for ref2 in refs:\n",
    "                ref_str2 = \"|\".join( str(x) for x in ref2 )\n",
    "\n",
    "                if ref_str2 <= ref_str:\n",
    "                    continue\n",
    "                if ref_str2 not in allowed_refs:\n",
    "                    continue\n",
    "\n",
    "                cnt((ref_str,ref_str2), 'cc', uid)\n",
    "                refcount += 1\n",
    "                if refcount % 10000 == 0:\n",
    "                    print(\"%s cocitations logged\" % refcount)\n",
    "                    \n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trim counters, excluding cited works with only a single reference\n",
    "\n",
    "Because of the huge number of cited works that only occur once, for efficiency it's best to simply log this statistic and eliminate them from the counter.\n",
    "\n",
    "\n",
    "*This might not be necessary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimCounters = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trimCounters:\n",
    "\n",
    "    onlyOne = len([x for x in cnt_doc['c'] if cnt_doc['c'][x] == 1])\n",
    "    total = len(cnt_doc['c'])\n",
    "    print(\"Of the %s cited works, %s (%0.2f%%) have only a single citation\" % (\n",
    "        total, onlyOne,\n",
    "        100 * onlyOne/total\n",
    "    ))\n",
    "\n",
    "    terms = list(cnt_doc['c'].keys())\n",
    "    counts = np.array([cnt_doc['c'][k] for k in terms])\n",
    "\n",
    "    cutoff = 2\n",
    "    to_remove = set([terms[int(i)] for i in np.argwhere(counts < cutoff)])\n",
    "    print(\"consolidating\", len(to_remove), list(to_remove)[:5])\n",
    "\n",
    "    cnt_doc.keys()\n",
    "\n",
    "    print(\"old size:\", len(cnt_doc['c']))\n",
    "    for tr in to_remove:\n",
    "        del cnt_doc['c'][tr]\n",
    "        del cnt_ind['c'][tr]\n",
    "    print(\"new size:\", len(cnt_doc['c']))\n",
    "\n",
    "    print(\"old size:\", len(cnt_doc['cy']))\n",
    "    tydels = [x for x in cnt_doc['cy'] if x[0] in to_remove]\n",
    "    for tydel in tydels:\n",
    "        del cnt_doc['cy'][tydel]\n",
    "        del cnt_ind['cy'][tydel]\n",
    "    print(\"new size:\", len(cnt_doc['cy']))\n",
    "\n",
    "    print(\"old size:\", len(cnt_doc['jc']))\n",
    "    jtdels = [x for x in cnt_doc['jc'] if x[1] in to_remove]\n",
    "    for jtdel in jtdels:\n",
    "        del cnt_doc['jc'][jtdel]\n",
    "        del cnt_ind['jc'][jtdel]\n",
    "    print(\"new size:\", len(cnt_doc['jc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export generated variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved sociology-wos.pubyears\n"
     ]
    }
   ],
   "source": [
    "# retrieve and use the MOST COMMON pub date for each\n",
    "pubyears = {\n",
    "    k:max(s.keys(), key=lambda x:multi_year[k][x]) for k,s in multi_year.items()\n",
    "    if len(s)\n",
    "}\n",
    "varname = \"%s.pubyears\"%database_name\n",
    "save_variable(varname, pubyears)\n",
    "print(\"saved %s\" % varname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving sociology-wos.doc ___ fj\n",
      "Saving sociology-wos.doc ___ fy\n",
      "Saving sociology-wos.doc ___ ty\n",
      "Saving sociology-wos.doc ___ fy.ty\n",
      "Saving sociology-wos.doc ___ fj.ty\n",
      "Saving sociology-wos.doc ___ fj.fy\n",
      "Saving sociology-wos.doc ___ a\n",
      "Saving sociology-wos.doc ___ a.fy\n",
      "Saving sociology-wos.doc ___ a.fj\n",
      "Saving sociology-wos.doc ___ a.fj.fy\n",
      "Saving sociology-wos.doc ___ a.c\n",
      "Saving sociology-wos.doc ___ c\n",
      "Saving sociology-wos.doc ___ c.fy\n",
      "Saving sociology-wos.doc ___ c.fj\n",
      "Saving sociology-wos.doc ___ c.fy.j\n"
     ]
    }
   ],
   "source": [
    "save_cnt(\"%s.doc\"%database_name, cnt_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving sociology-wos.ind ___ fj\n",
      "Saving sociology-wos.ind ___ fy\n",
      "Saving sociology-wos.ind ___ ty\n",
      "Saving sociology-wos.ind ___ fy.ty\n",
      "Saving sociology-wos.ind ___ fj.ty\n",
      "Saving sociology-wos.ind ___ fj.fy\n",
      "Saving sociology-wos.ind ___ a\n",
      "Saving sociology-wos.ind ___ a.fy\n",
      "Saving sociology-wos.ind ___ a.fj\n",
      "Saving sociology-wos.ind ___ a.fj.fy\n",
      "Saving sociology-wos.ind ___ a.c\n",
      "Saving sociology-wos.ind ___ c\n",
      "Saving sociology-wos.ind ___ c.fy\n",
      "Saving sociology-wos.ind ___ c.fj\n",
      "Saving sociology-wos.ind ___ c.fy.j\n"
     ]
    }
   ],
   "source": [
    "save_cnt(\"%s.ind\"%database_name, cnt_ind)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (qualitative analysis of literature)",
   "language": "python",
   "name": "pycharm-b9c7981b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
